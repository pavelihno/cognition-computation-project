{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ab744935",
      "metadata": {
        "id": "ab744935"
      },
      "source": [
        "# Cognition and Computation Project"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "973173fb",
      "metadata": {
        "id": "973173fb"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "039de992",
      "metadata": {
        "id": "039de992"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import json\n",
        "import random\n",
        "import requests\n",
        "\n",
        "from IPython.display import display, clear_output\n",
        "from urllib.parse import quote\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf35dbdf",
      "metadata": {
        "id": "cf35dbdf"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "[The Quick, Draw! Dataset](https://github.com/googlecreativelab/quickdraw-dataset)\n",
        "\n",
        "```quote\n",
        "The Quick Draw Dataset is a collection of 50 million drawings across 345 categories, contributed by players of the game Quick, Draw!. The drawings were captured as timestamped vectors, tagged with metadata including what the player was asked to draw and in which country the player was located.\n",
        "```\n",
        "\n",
        "### Image Categories/Classes\n",
        "\n",
        "Original 345 categories list was divided into 10 broad categories that are stored in the `categories.json` file. Broad categories are used for hierarchical sampling during dataset creation. For predicting the image class original categories are used, however, classification readout layers can be modified to predict broad categories as well.\n",
        "\n",
        "For this project only 20 randomly selected sub-categories from different broad categories are used with 2000 samples per sub-category. Performance of the model may vary with more selective choice of sub-categories and different number of sub-categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61e97600",
      "metadata": {
        "id": "61e97600"
      },
      "outputs": [],
      "source": [
        "DATASET_BASE_URL = \"https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/\"\n",
        "IMG_SHAPE = (28, 28)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "CLASS_NUM = 20\n",
        "SAMPLES_PER_CLASS = 2000\n",
        "\n",
        "categories_json_filename = 'categories.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c8cce620",
      "metadata": {
        "id": "c8cce620"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# Downloading categories\n",
        "!wget -O {categories_json_filename} \"https://raw.githubusercontent.com/pavelihno/cognition-computation-project/master/data/{categories_json_filename}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1b1d211d",
      "metadata": {
        "id": "1b1d211d"
      },
      "outputs": [],
      "source": [
        "categories_json = {}\n",
        "with open(categories_json_filename, 'r') as f:\n",
        "    categories_json = json.load(f)\n",
        "\n",
        "display(categories_json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "116f5442",
      "metadata": {
        "id": "116f5442"
      },
      "outputs": [],
      "source": [
        "def get_quickdraw_data(categories_json, total_classes=5, samples_per_class=2000, seed=123):\n",
        "    \"\"\"\n",
        "    categories_json: dict {SuperCategory: [list of subcategories]}\n",
        "    total_classes: total number of classes to sample\n",
        "    samples_per_class: number of samples to download per class\n",
        "    \"\"\"\n",
        "    selected_classes = []\n",
        "\n",
        "    random.seed(seed)\n",
        "\n",
        "    # 1. Hierarchical Sampling\n",
        "    pool = {k: list(v) for k, v in categories_json.items()}\n",
        "    while len(selected_classes) < total_classes:\n",
        "        broad_cats = [c for c, items in pool.items() if len(items) > 0]\n",
        "        if not broad_cats:\n",
        "            break\n",
        "\n",
        "        # Uniformly select broad category\n",
        "        broad_cat = random.choice(broad_cats)\n",
        "\n",
        "        # Uniformly sample sub-category\n",
        "        sub_cat = random.choice(pool[broad_cat])\n",
        "\n",
        "        selected_classes.append((sub_cat, broad_cat))\n",
        "\n",
        "        # Exclude sampled sub-category\n",
        "        pool[broad_cat].remove(sub_cat)\n",
        "\n",
        "    # 2. Downloading data\n",
        "    X, Y, Y_super = [], [], []\n",
        "\n",
        "    for i, (sub_cat, broad_cat) in enumerate(selected_classes):\n",
        "        file_name = f'{quote(sub_cat)}.npy'\n",
        "        url = DATASET_BASE_URL + file_name\n",
        "\n",
        "        print(f'Downloading {sub_cat} ({broad_cat})...')\n",
        "        res = requests.get(url)\n",
        "        data = np.frombuffer(\n",
        "            res.content, dtype=np.uint8, offset=80\n",
        "        )\n",
        "        data = data.reshape(-1, math.prod(IMG_SHAPE))[:samples_per_class]\n",
        "\n",
        "        X.append(data)\n",
        "        Y.append(np.full(len(data), i))  # Sub-class label (from 0 to total_classes-1)\n",
        "\n",
        "        # Numerical label for broad category\n",
        "        Y_super.append(\n",
        "            [list(categories_json.keys()).index(broad_cat)] * len(data)\n",
        "        )\n",
        "\n",
        "    X = np.concatenate(X) / 255.0  # Normalize [0,1]\n",
        "    Y = np.concatenate(Y)\n",
        "\n",
        "    return torch.FloatTensor(X), torch.LongTensor(Y), selected_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3d3edf5",
      "metadata": {
        "id": "b3d3edf5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# Downloading previously used data\n",
        "quickdraw_data_filename = 'quickdraw_data.npz'\n",
        "\n",
        "!wget -O {quickdraw_data_filename} \"https://raw.githubusercontent.com/pavelihno/cognition-computation-project/master/data/{quickdraw_data_filename}\"\n",
        "\n",
        "loaded_data = np.load(quickdraw_data_filename, allow_pickle=True)\n",
        "\n",
        "sketch_data = torch.from_numpy(loaded_data['data'])\n",
        "sketch_labels = torch.from_numpy(loaded_data['labels'])\n",
        "sketch_categories = loaded_data['categories'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e585ac2b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e585ac2b",
        "outputId": "46a542ee-af76-4724-8de7-9c3749c01501"
      },
      "outputs": [],
      "source": [
        "# Sampling new data\n",
        "sketch_data, sketch_labels, sketch_categories = get_quickdraw_data(\n",
        "    categories_json, total_classes=CLASS_NUM, samples_per_class=SAMPLES_PER_CLASS, seed=RANDOM_SEED\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7646140c",
      "metadata": {
        "id": "7646140c"
      },
      "outputs": [],
      "source": [
        "# Saving sampled data\n",
        "quickdraw_data_filename = 'quickdraw_data.npz'\n",
        "\n",
        "save_data = sketch_data.numpy()\n",
        "save_labels = sketch_labels.numpy()\n",
        "save_categories = np.array(sketch_categories, dtype=object)\n",
        "\n",
        "np.savez_compressed(\n",
        "    quickdraw_data_filename,\n",
        "    data=save_data,\n",
        "    labels=save_labels,\n",
        "    categories=save_categories\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ef785e9e",
      "metadata": {
        "id": "ef785e9e"
      },
      "outputs": [],
      "source": [
        "def show_img(\n",
        "    img, sub_category=None, broad_category=None, predicted_sub_category=None, predicted_broad_category=None\n",
        "):\n",
        "    if sub_category is not None:\n",
        "        print(f'Sub-category: {sub_category}')\n",
        "    if broad_category is not None:\n",
        "        print(f'Broad category: {broad_category}')\n",
        "    if predicted_sub_category is not None:\n",
        "        print(f'Predicted sub-category: {predicted_sub_category}')\n",
        "    if predicted_broad_category is not None:\n",
        "        print(f'Predicted broad category: {predicted_broad_category}')\n",
        "    img = img.cpu()\n",
        "    plt.imshow(img, cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def compare_imgs(imgs, sub_category, broad_category):\n",
        "    print(f'Sub-category: {sub_category}')\n",
        "    print(f'Broad category: {broad_category}')\n",
        "    n_imgs = len(imgs)\n",
        "    for i, img in enumerate(imgs):\n",
        "        img = img.cpu()\n",
        "        plt.subplot(1, n_imgs, i+1)\n",
        "        plt.imshow(img, cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def show_class_distribution(labels, categories):\n",
        "    counts = pd.Series(labels.numpy()).value_counts().sort_index()\n",
        "\n",
        "    df_meta = pd.DataFrame(\n",
        "        categories, columns=['sub_category', 'broad_category']\n",
        "    )\n",
        "    df_meta['samples'] = counts.values\n",
        "\n",
        "    sns.set_theme(style='whitegrid')\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
        "\n",
        "    sns.barplot(\n",
        "        data=df_meta,\n",
        "        x='sub_category', y='samples', hue='broad_category',\n",
        "        palette='viridis', dodge=False, legend=False, ax=axes[0]\n",
        "    )\n",
        "    axes[0].set_title('Samples per Sub-Category')\n",
        "    axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    sns.barplot(\n",
        "        data=df_meta.groupby('broad_category')['samples'].sum().reset_index(),\n",
        "        x='broad_category', y='samples', hue='broad_category',\n",
        "        palette='magma', legend=False, ax=axes[1]\n",
        "    )\n",
        "    axes[1].set_title('Samples per Broad Category')\n",
        "    axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "727eef88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "727eef88",
        "outputId": "3082c210-48c0-48f0-f936-0acc262f168f"
      },
      "outputs": [],
      "source": [
        "img_idx = 1501\n",
        "img = sketch_data[img_idx]\n",
        "label = sketch_labels[img_idx]\n",
        "sub_category, broad_category = sketch_categories[label]\n",
        "\n",
        "show_img(\n",
        "    img.reshape(IMG_SHAPE),\n",
        "    sub_category=sub_category,\n",
        "    broad_category=broad_category\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "cd6b68dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "cd6b68dc",
        "outputId": "f77c2a07-2190-4f0b-f6b6-04301a5501ee"
      },
      "outputs": [],
      "source": [
        "show_class_distribution(sketch_labels, sketch_categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "fd12fefb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd12fefb",
        "outputId": "de315a2a-248c-4f39-bf18-692af3493ca4"
      },
      "outputs": [],
      "source": [
        "# Create train and test dataloaders\n",
        "train_size = int(0.8 * len(sketch_data))\n",
        "test_size = len(sketch_data) - train_size\n",
        "\n",
        "train_data, test_data = torch.utils.data.random_split(\n",
        "    sketch_data, [train_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(RANDOM_SEED)\n",
        ")\n",
        "\n",
        "train_labels, test_labels = torch.utils.data.random_split(\n",
        "    sketch_labels, [train_size, test_size],\n",
        "    generator=torch.Generator().manual_seed(RANDOM_SEED)\n",
        ")\n",
        "\n",
        "train_dataset = TensorDataset(\n",
        "    train_data.dataset[train_data.indices],\n",
        "    train_labels.dataset[train_labels.indices]\n",
        ")\n",
        "test_dataset = TensorDataset(\n",
        "    test_data.dataset[test_data.indices],\n",
        "    test_labels.dataset[test_labels.indices]\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "print(f'Training dataset size: {len(train_dataset)}')\n",
        "print(f'Test dataset size: {len(test_dataset)}')\n",
        "\n",
        "device_name = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device = torch.device(device_name)\n",
        "\n",
        "print(f'Using device: {device_name}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7e72983",
      "metadata": {
        "id": "e7e72983"
      },
      "source": [
        "## Training\n",
        "\n",
        "Universal utils functions for training and loading the Pytorch models. Train function supports both self-supervised (for VAE) and supervised training (for classification linear readout)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "4cc5afa4",
      "metadata": {
        "id": "4cc5afa4"
      },
      "outputs": [],
      "source": [
        "def train_model(\n",
        "    model, train_loader, optimizer_cls, loss_fn, optimizer_params={}, self_supervised=False, epochs=10,\n",
        "    device=None, filename=None, checkpoint_freq=5, checkpoint_dir=None, checkpoint_path=None\n",
        "):\n",
        "    model.to(device)\n",
        "\n",
        "    start_epoch = 0\n",
        "\n",
        "    optimizer = optimizer_cls(model.parameters(), **optimizer_params)\n",
        "\n",
        "    if checkpoint_path is not None:\n",
        "        if os.path.exists(checkpoint_path):\n",
        "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            start_epoch = checkpoint['epoch']\n",
        "            loss = checkpoint['loss']\n",
        "\n",
        "            # Override optimizer params\n",
        "            for group in optimizer.param_groups:\n",
        "                for k, v in optimizer_params.items():\n",
        "                    if k != 'params':\n",
        "                        group[k] = v\n",
        "\n",
        "            print(f'Resumed from epoch {start_epoch}, Loss: {loss:.4f}')\n",
        "\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        model.train()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        n_samples = 0\n",
        "\n",
        "        for data, targets in train_loader:\n",
        "            data = data.to(device)\n",
        "            targets = targets.to(device) if not self_supervised else data.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(data)\n",
        "            loss = loss_fn(outputs, targets)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            n_samples += data.size(0)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        epoch_loss = running_loss / n_samples\n",
        "\n",
        "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}')\n",
        "\n",
        "        if checkpoint_dir and (epoch + 1) % checkpoint_freq == 0:\n",
        "            checkpoint_path = os.path.join(\n",
        "                checkpoint_dir,\n",
        "                f'{filename}_check_{epoch+1}.pth'\n",
        "            )\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': epoch_loss,\n",
        "            }, checkpoint_path)\n",
        "            print(f'Checkpoint saved: {checkpoint_path}')\n",
        "\n",
        "    if filename is not None:\n",
        "        file_path = f'{filename}.pth'\n",
        "        torch.save(model.state_dict(), file_path)\n",
        "        print(f'Model weights saved to \"{file_path}\"')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_model(model, file_path, download=True, from_checkpoint=False, device=None):\n",
        "\n",
        "    if download:\n",
        "        !wget -O {file_path} \"https://raw.githubusercontent.com/pavelihno/cognition-computation-project/master/models/{file_path}\"\n",
        "\n",
        "    state_file = torch.load(file_path, map_location=device)\n",
        "    if from_checkpoint:\n",
        "        model.load_state_dict(state_file['model_state_dict'])\n",
        "    else:\n",
        "        model.load_state_dict(state_file)\n",
        "\n",
        "    model.to(device)\n",
        "    print(f'Model weights loaded from \"{file_path}\"')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a794082d",
      "metadata": {
        "id": "a794082d"
      },
      "source": [
        "## Variational Autoencoder (VAE)\n",
        "\n",
        "[An Introduction to Variational Autoencoders. Diederik P. Kingma, Max Welling](https://arxiv.org/pdf/1906.02691)\n",
        "\n",
        "![VAE Architecture](https://upload.wikimedia.org/wikipedia/commons/1/11/Reparameterized_Variational_Autoencoder.png)\n",
        "\n",
        "\n",
        "Implemented VAE consists of an encoder and a decoder with fully connected layers. Different combinations of number of layers and hidden units were experimented with. Improvements in reconstruction loss were observed with increasing number of layers of the encoder.\n",
        "\n",
        "Also strategy with weak decoder can be used to force the encoder to learn better latent representations.\n",
        "\n",
        "\n",
        "### Objective Function\n",
        "\n",
        "$$ELBO = \\underbrace{\\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)]}_{\\text{Reconstruction Log-Likelihood}} - \\underbrace{D_{KL}(q_\\phi(z|x) || p(z))}_{\\text{KL Divergence Regularizer}}$$\n",
        "\n",
        "KL Divergence Regularizer\n",
        "\n",
        "$$D_{KL} = -\\frac{1}{2} \\sum_{j=1}^J (1 + \\log(\\sigma_j^2) - \\mu_j^2 - \\sigma_j^2)$$\n",
        "\n",
        "The goal of a VAE is to maximize the ELBO. In practice, we minimize the Negative ELBO as a loss function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "32cdb17f",
      "metadata": {
        "id": "32cdb17f"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.fc_mu = nn.Linear(hidden_dim // 4, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dim // 4, latent_dim)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim // 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 4, input_dim),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        mu = self.fc_mu(h)\n",
        "        logvar = self.fc_logvar(h)\n",
        "        return mu, logvar\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        recon_x = self.decode(z)\n",
        "        return recon_x, mu, logvar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "7e4ee7da",
      "metadata": {
        "id": "7e4ee7da"
      },
      "outputs": [],
      "source": [
        "def recon_log_likelihood(recon_x, x):\n",
        "    return -F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
        "\n",
        "\n",
        "def kl_divergence(mu, logvar):\n",
        "    return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "\n",
        "def elbo(recon_x, x, mu, logvar):\n",
        "    return recon_log_likelihood(recon_x, x) - kl_divergence(mu, logvar)\n",
        "\n",
        "\n",
        "def vae_loss_fn(outputs, x):\n",
        "    recon_x, mu, logvar = outputs\n",
        "    return -elbo(recon_x, x, mu, logvar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "39dbf5ba",
      "metadata": {
        "id": "39dbf5ba"
      },
      "outputs": [],
      "source": [
        "vae_model = VAE(\n",
        "    input_dim=IMG_SHAPE[0] * IMG_SHAPE[1],\n",
        "    hidden_dim=512,\n",
        "    latent_dim=32\n",
        ")\n",
        "optimizer_cls = torch.optim.Adam\n",
        "optimizer_params = {'lr': 0.001}\n",
        "\n",
        "num_linear_layers = len([m for m in vae_model.encoder if isinstance(m, nn.Linear)]) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "75elPsYKMKvL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75elPsYKMKvL",
        "outputId": "3ee2d461-c62b-49f1-ddb0-67f6c068eed6"
      },
      "outputs": [],
      "source": [
        "# Training VAE\n",
        "vae_model = train_model(\n",
        "    model=vae_model,\n",
        "    train_loader=train_loader,\n",
        "    optimizer_cls=optimizer_cls,\n",
        "    optimizer_params=optimizer_params,\n",
        "    loss_fn=vae_loss_fn,\n",
        "    self_supervised=True,\n",
        "    epochs=500,\n",
        "    device=device,\n",
        "    filename='vae_model',\n",
        "    checkpoint_freq=100,\n",
        "    checkpoint_dir='checkpoints',\n",
        "    # checkpoint_path='checkpoints/vae.pth_check_250.pth'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "1a23add8",
      "metadata": {
        "id": "1a23add8",
        "outputId": "7f6a06c6-dd97-456d-f7ba-4804cd6e9c25"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# Loading VAE\n",
        "vae_model = load_model(\n",
        "    model=vae_model,\n",
        "    file_path='vae_model.pth',\n",
        "    download=True,\n",
        "    from_checkpoint=False,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "qk6zdceKNxnR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "id": "qk6zdceKNxnR",
        "outputId": "54165943-5aab-4c7f-b6b0-d2ac9ec64442"
      },
      "outputs": [],
      "source": [
        "test_idx = 1000\n",
        "test_img, test_label = test_dataset[test_idx]\n",
        "test_sub_category, test_broad_category = sketch_categories[test_label]\n",
        "\n",
        "recon_img, _, _ = vae_model(test_img.to(device))\n",
        "\n",
        "compare_imgs(\n",
        "    [img.reshape(IMG_SHAPE) for img in [test_img, recon_img.detach()]],\n",
        "    sub_category=test_sub_category,\n",
        "    broad_category=test_broad_category\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba16b654",
      "metadata": {
        "id": "ba16b654"
      },
      "source": [
        "## Visualizing receptive fields\n",
        "\n",
        "To visualize receptive fields in a VAE, we use a linear projection method that collapses the multi-layer encoder into a single weight matrix. This allows to build projection from each hidden layer to input pixels.\n",
        "\n",
        "Layer 1. Some receptive fields act as edge detectors. They respond to simple local shapes, forms in specific parts of the image.\n",
        "\n",
        "Layer 2. These fields are more complex and combine the edges from Layer 1 into parts or shapes.\n",
        "\n",
        "Layer 3. These are global templates that may look like entire objects. They represent the \"concepts\" the VAE has learned from training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "0192d9ea",
      "metadata": {
        "id": "0192d9ea"
      },
      "outputs": [],
      "source": [
        "def get_effective_weights(model, layer_idx=None):\n",
        "    weights = []\n",
        "\n",
        "    # Input -> Linear -> fc_mu -> Latent\n",
        "    for m in model.encoder:\n",
        "        if isinstance(m, nn.Linear):\n",
        "            weights.append(m.weight.data)\n",
        "    weights.append(model.fc_mu.weight.data)\n",
        "\n",
        "    eff_w = weights[0]\n",
        "    for i in range(1, layer_idx + 1):\n",
        "        eff_w = torch.mm(weights[i], eff_w)\n",
        "\n",
        "    return eff_w   # [input_dim, latent_dim]\n",
        "\n",
        "\n",
        "def plot_fields(weights, title, img_shape, max_plots=16):\n",
        "    total_available = weights.shape[0]\n",
        "    n_plots = min(total_available, max_plots)\n",
        "\n",
        "    n_cols = math.ceil(math.sqrt(n_plots))\n",
        "    n_rows = math.ceil(n_plots / n_cols)\n",
        "\n",
        "    fig, axs = plt.subplots(\n",
        "        n_rows, n_cols, figsize=(n_cols * 2.5, n_rows * 2.5)\n",
        "    )\n",
        "    fig.suptitle(title, fontsize=16, y=1.01)\n",
        "\n",
        "    for i in range(n_plots):\n",
        "        ax = axs[i // n_cols, i % n_cols]\n",
        "\n",
        "        field = weights[i, :].view(img_shape).cpu().numpy()\n",
        "\n",
        "        im = ax.imshow(field, cmap='RdBu_r')\n",
        "        ax.axis('off')\n",
        "        ax.set_title(f'{i}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "842d1a5c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "842d1a5c",
        "outputId": "11d15085-bf0c-4cf6-fd23-13d78eb8c012"
      },
      "outputs": [],
      "source": [
        "for layer_idx in range(num_linear_layers):\n",
        "    enc_w = get_effective_weights(\n",
        "        vae_model, layer_idx=layer_idx\n",
        "    )\n",
        "    plot_fields(\n",
        "        enc_w, f'Encoder Receptive Fields (Layer {layer_idx+1})', IMG_SHAPE, max_plots=100\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d06219b5",
      "metadata": {
        "id": "d06219b5"
      },
      "source": [
        "## Clustering internal representations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "573d369a",
      "metadata": {
        "id": "573d369a"
      },
      "outputs": [],
      "source": [
        "def get_mean_representations(representations, labels):\n",
        "    return {\n",
        "        label: np.mean(representations[labels == label], axis=0)\n",
        "        for label in np.unique(labels)\n",
        "    }\n",
        "\n",
        "\n",
        "def show_clusters(\n",
        "    representations, labels, categories\n",
        "):\n",
        "    mean_representations = get_mean_representations(representations, labels)\n",
        "    unique_labels = sorted(mean_representations.keys())\n",
        "    representations_matrix = np.concatenate([\n",
        "        np.expand_dims(mean_representations[label], axis=0)\n",
        "        for label in unique_labels\n",
        "    ])\n",
        "\n",
        "    pca = PCA(n_components=2)\n",
        "    pca_result = pca.fit_transform(representations_matrix)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(12, 10))\n",
        "    plt.scatter(pca_result[:, 0], pca_result[:, 1], s=100)\n",
        "\n",
        "    for i, label in enumerate(unique_labels):\n",
        "        sub_category = categories[label][0]\n",
        "        plt.annotate(\n",
        "            sub_category,\n",
        "            (pca_result[i, 0], pca_result[i, 1]),\n",
        "            xytext=(10, 10),\n",
        "            textcoords='offset points',\n",
        "            fontsize=10,\n",
        "            bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7)\n",
        "        )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def show_dendrogram(\n",
        "    representations, labels, categories\n",
        "):\n",
        "    mean_representations = get_mean_representations(representations, labels)\n",
        "    unique_labels = sorted(mean_representations.keys())\n",
        "    representations_matrix = np.concatenate([\n",
        "        np.expand_dims(mean_representations[label], axis=0)\n",
        "        for label in unique_labels\n",
        "    ])\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(14, 8))\n",
        "    linkage = scipy.cluster.hierarchy.linkage(\n",
        "        representations_matrix, method='complete'\n",
        "    )\n",
        "\n",
        "    dendrogram_labels = [categories[label][0] for label in unique_labels]\n",
        "\n",
        "    dendrogram = scipy.cluster.hierarchy.dendrogram(\n",
        "        linkage,\n",
        "        labels=dendrogram_labels,\n",
        "        leaf_rotation=45,\n",
        "        leaf_font_size=12\n",
        "    )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "647cb871",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "647cb871",
        "outputId": "0a6917e3-51e5-47a5-f6dc-8246f2dbe38d"
      },
      "outputs": [],
      "source": [
        "vae_model.eval()\n",
        "\n",
        "all_latent_representations = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, labels in test_loader:\n",
        "        data = data.to(device)\n",
        "        mu, logvar = vae_model.encode(data)\n",
        "        all_latent_representations.append(mu.cpu().numpy())\n",
        "        all_labels.append(labels.numpy())\n",
        "\n",
        "latent_representations = np.concatenate(all_latent_representations, axis=0)\n",
        "test_labels_np = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "print('======= Latent Space =======')\n",
        "show_clusters(latent_representations, test_labels_np, sketch_categories)\n",
        "show_dendrogram(latent_representations, test_labels_np, sketch_categories)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b366ad1d",
      "metadata": {
        "id": "b366ad1d"
      },
      "source": [
        "## Linear read-out layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "de27f85c",
      "metadata": {
        "id": "de27f85c"
      },
      "outputs": [],
      "source": [
        "class LinearReadOutLayer(nn.Module):\n",
        "    def __init__(self, n_input, n_output=10) -> None:\n",
        "        super(LinearReadOutLayer, self).__init__()\n",
        "\n",
        "        self.linear = nn.Linear(n_input, n_output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "\n",
        "class LinearReadOutWrapper(nn.Module):\n",
        "    def __init__(self, vae, linear_idx, n_output):\n",
        "        super(LinearReadOutWrapper, self).__init__()\n",
        "\n",
        "        self.vae = vae\n",
        "        self.linear_idx = linear_idx\n",
        "\n",
        "        # All linear layers of the encoder\n",
        "        self.enc_linears = [m for m in vae.encoder if isinstance(m, nn.Linear)]\n",
        "        all_probes = self.enc_linears + [vae.fc_mu]\n",
        "\n",
        "        # Dimensions of the specific linear layer\n",
        "        selected_layer = all_probes[linear_idx]\n",
        "        n_input = selected_layer.out_features\n",
        "\n",
        "        self.readout = LinearReadOutLayer(n_input, n_output)\n",
        "\n",
        "        # Freezing VAE params\n",
        "        for param in self.vae.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        feat = x\n",
        "        linear_count = 0\n",
        "\n",
        "        for module in self.vae.encoder:\n",
        "            feat = module(feat)\n",
        "            if isinstance(module, nn.Linear):\n",
        "                if linear_count == self.linear_idx:\n",
        "                    return self.readout(feat)\n",
        "                linear_count += 1\n",
        "\n",
        "        feat = self.vae.fc_mu(feat)\n",
        "        return self.readout(feat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "T4swZr21tsg5",
      "metadata": {
        "id": "T4swZr21tsg5"
      },
      "outputs": [],
      "source": [
        "linear_layers = []\n",
        "\n",
        "for layer_idx in range(num_linear_layers):\n",
        "    linear_readout_wrapper = LinearReadOutWrapper(\n",
        "        vae=vae_model,\n",
        "        linear_idx=layer_idx,\n",
        "        n_output=len(sketch_categories)\n",
        "    ).to(device)\n",
        "\n",
        "    linear_layers.append({\n",
        "        'linear_readout': linear_readout_wrapper,\n",
        "        'filename': f'linear_readout_{layer_idx+1}'\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "037691a3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "037691a3",
        "outputId": "d4add81d-c736-4e3f-830d-5cd33999f2d1"
      },
      "outputs": [],
      "source": [
        "for layer_idx, linear_layer_dict in enumerate(linear_layers):\n",
        "    print(f'======= Layer {layer_idx + 1} =======')\n",
        "\n",
        "    linear_readout_wrapper = linear_layer_dict['linear_readout']\n",
        "    filename = linear_layer_dict['filename']\n",
        "\n",
        "    train_model(\n",
        "        model=linear_readout_wrapper,\n",
        "        train_loader=train_loader,\n",
        "        optimizer_cls=torch.optim.Adam,\n",
        "        optimizer_params={'lr': 0.001},\n",
        "        loss_fn=nn.CrossEntropyLoss(),\n",
        "        epochs=200,\n",
        "        device=device,\n",
        "        filename=filename\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "JDvtGzF1u3UH",
      "metadata": {
        "id": "JDvtGzF1u3UH"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "# Downloading and loading weights from file\n",
        "_loaded_linear_layers = []\n",
        "\n",
        "for linear_layer_dict in linear_layers:\n",
        "    linear_readout_wrapper = linear_layer_dict['linear_readout']\n",
        "    filename = linear_layer_dict['filename']\n",
        "    file_path = f'{filename}.pth'\n",
        "\n",
        "    !wget -O {file_path} \"https://raw.githubusercontent.com/pavelihno/cognition-computation-project/master/models/{file_path}\"\n",
        "\n",
        "    linear_readout_wrapper.load_state_dict(torch.load(file_path, map_location=device))\n",
        "    linear_readout_wrapper.to(device)\n",
        "\n",
        "    linear_layer_dict['linear_readout'] = linear_readout_wrapper\n",
        "    _loaded_linear_layers.append(linear_layer_dict)\n",
        "\n",
        "linear_layers = _loaded_linear_layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "3c28dbc3",
      "metadata": {
        "id": "3c28dbc3"
      },
      "outputs": [],
      "source": [
        "def get_accuracy(model, test_loader, device=None):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, labels in test_loader:\n",
        "            data = data.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def show_confusion_matrix(model, test_loader, categories, device=None):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, labels in test_loader:\n",
        "            data = data.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(\n",
        "        cm, annot=True, fmt='d', cmap='Blues',\n",
        "        xticklabels=[c[0] for c in categories],\n",
        "        yticklabels=[c[0] for c in categories]\n",
        "    )\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "e47aadcf",
      "metadata": {
        "id": "e47aadcf",
        "outputId": "087dbfb2-1a4f-4103-da1d-3b5771e749f1"
      },
      "outputs": [],
      "source": [
        "linear_accuracies = []\n",
        "\n",
        "for layer_idx, linear_layer_dict in enumerate(linear_layers):\n",
        "    linear_readout_wrapper = linear_layer_dict['linear_readout']\n",
        "\n",
        "    accuracy = get_accuracy(\n",
        "        model=linear_readout_wrapper,\n",
        "        test_loader=test_loader,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    linear_accuracies.append(accuracy)\n",
        "\n",
        "    print(f'Layer {layer_idx + 1}: Test Accuracy: {accuracy:.4f}')\n",
        "\n",
        "    show_confusion_matrix(\n",
        "        model=linear_readout_wrapper,\n",
        "        test_loader=test_loader,\n",
        "        categories=sketch_categories,\n",
        "        device=device\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77a35a43",
      "metadata": {},
      "source": [
        "## Robustness to noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fdaaaa5",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ed07e096",
      "metadata": {},
      "source": [
        "## Adversarial attacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59bb93af",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
